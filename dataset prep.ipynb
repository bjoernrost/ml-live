{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we stage some data in GCS buckets for cloud-based training. We could do all of this in the main notebook but it's not too relevant for the demo itself, so we are breaking it out and only running it once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup GCP environment\n",
    "\n",
    "* Your project id is the *unique* string that identifies your project (not the project name). You can find this from the GCP Console dashboard's Home page.  My dashboard reads:  <b>Project ID:</b> cloud-training-demos\n",
    "* Cloud training often involves saving and restoring model files. Therefore, we should <b>create a single-region bucket</b>. If you don't have a bucket already, I suggest that you create one from the GCP console (because it will dynamically check whether the bucket name you want is available)\n",
    "\n",
    "<b>Change the cell below</b> to reflect your Project ID and bucket name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /usr/local/envs/py2env\n",
      "\n",
      "  added / updated specs: \n",
      "    - pytz\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2018.10.15         |           py27_0         139 KB  defaults\n",
      "    pytz-2018.5                |           py27_0         231 KB  defaults\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         369 KB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    certifi: 2018.8.13-py27_0 defaults --> 2018.10.15-py27_0 defaults\n",
      "    pytz:    2016.7-py27_0    defaults --> 2018.5-py27_0     defaults\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Uninstalling google-cloud-dataflow-2.0.0:\n",
      "  Successfully uninstalled google-cloud-dataflow-2.0.0\n",
      "Collecting apache-beam[gcp]\n",
      "  Using cached https://files.pythonhosted.org/packages/e8/5c/4c4302f48686e89468cc036a8171b55a2f509339dd5d1613be2ec5c41c8e/apache_beam-2.7.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Requirement already satisfied, skipping upgrade: dill<=0.2.8.2,>=0.2.6 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (0.2.6)\n",
      "Requirement already satisfied, skipping upgrade: oauth2client<5,>=2.0.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (2.2.0)\n",
      "Collecting fastavro==0.19.7 (from apache-beam[gcp])\n",
      "Collecting typing<3.7.0,>=3.6.0 (from apache-beam[gcp])\n",
      "  Using cached https://files.pythonhosted.org/packages/cc/3e/29f92b7aeda5b078c86d14f550bf85cff809042e3429ace7af6193c3bc9f/typing-3.6.6-py2-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: pyyaml<4.0.0,>=3.12 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (3.13)\n",
      "Requirement already satisfied, skipping upgrade: six<1.12,>=1.9 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (1.10.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf<4,>=3.5.0.post1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (3.5.2)\n",
      "Requirement already satisfied, skipping upgrade: future<1.0.0,>=0.16.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: avro<2.0.0,>=1.8.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (1.8.2)\n",
      "Requirement already satisfied, skipping upgrade: crcmod<2.0,>=1.7 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (1.7)\n",
      "Requirement already satisfied, skipping upgrade: httplib2<=0.11.3,>=0.8 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (0.11.3)\n",
      "Requirement already satisfied, skipping upgrade: futures<4.0.0,>=3.1.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (3.2.0)\n",
      "Collecting pydot<1.3,>=1.2.0 (from apache-beam[gcp])\n",
      "Requirement already satisfied, skipping upgrade: grpcio<2,>=1.8 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (1.14.1)\n",
      "Collecting pyvcf<0.7.0,>=0.6.8 (from apache-beam[gcp])\n",
      "Requirement already satisfied, skipping upgrade: mock<3.0.0,>=1.0.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (2.0.0)\n",
      "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam[gcp])\n",
      "Collecting pytz<=2018.4,>=2018.3 (from apache-beam[gcp])\n",
      "  Using cached https://files.pythonhosted.org/packages/dc/83/15f7833b70d3e067ca91467ca245bae0f6fe56ddc7451aa0dc5606b120f2/pytz-2018.4-py2.py3-none-any.whl\n",
      "Collecting google-apitools<=0.5.20,>=0.5.18; extra == \"gcp\" (from apache-beam[gcp])\n",
      "  Using cached https://files.pythonhosted.org/packages/1d/0c/64f84f91643f775fdb64c6c10f4a4f0d827f8b0d98a2ba2b4bb9dc2f8646/google_apitools-0.5.20-py2-none-any.whl\n",
      "Collecting proto-google-cloud-pubsub-v1==0.15.4; extra == \"gcp\" (from apache-beam[gcp])\n",
      "Collecting google-cloud-bigquery==0.25.0; extra == \"gcp\" (from apache-beam[gcp])\n",
      "  Using cached https://files.pythonhosted.org/packages/76/67/6165c516ff6ceaa62eb61f11d8451e1b0acc4d3775e181630aba9652babb/google_cloud_bigquery-0.25.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-pubsub==0.26.0; extra == \"gcp\" (from apache-beam[gcp])\n",
      "  Using cached https://files.pythonhosted.org/packages/37/92/c74a643126d58505daec9addf872dfaffea3305981b90cc435f4b9213cdd/google_cloud_pubsub-0.26.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: googledatastore==7.0.1; extra == \"gcp\" in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (7.0.1)\n",
      "Requirement already satisfied, skipping upgrade: proto-google-cloud-datastore-v1<=0.90.4,>=0.90.0; extra == \"gcp\" in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (0.90.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.7 in /usr/local/envs/py2env/lib/python2.7/site-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.0.5 in /usr/local/envs/py2env/lib/python2.7/site-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/envs/py2env/lib/python2.7/site-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]) (3.4.2)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=18.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from fastavro==0.19.7->apache-beam[gcp]) (40.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.1.4 in /usr/local/envs/py2env/lib/python2.7/site-packages (from pydot<1.3,>=1.2.0->apache-beam[gcp]) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: enum34>=1.0.4 in /usr/local/envs/py2env/lib/python2.7/site-packages (from grpcio<2,>=1.8->apache-beam[gcp]) (1.1.6)\n",
      "Requirement already satisfied, skipping upgrade: funcsigs>=1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pbr>=0.11 in /usr/local/envs/py2env/lib/python2.7/site-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]) (4.2.0)\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.7.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]) (2.18.4)\n",
      "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp])\n",
      "Collecting fasteners>=0.14 (from google-apitools<=0.5.20,>=0.5.18; extra == \"gcp\"->apache-beam[gcp])\n",
      "  Using cached https://files.pythonhosted.org/packages/14/3a/096c7ad18e102d4f219f5dd15951f9728ca5092a3385d2e8f79a7c1e1017/fasteners-0.14.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.5.2 in /usr/local/envs/py2env/lib/python2.7/site-packages (from proto-google-cloud-pubsub-v1==0.15.4; extra == \"gcp\"->apache-beam[gcp]) (1.5.3)\n",
      "Collecting google-cloud-core<0.26dev,>=0.25.0 (from google-cloud-bigquery==0.25.0; extra == \"gcp\"->apache-beam[gcp])\n",
      "  Using cached https://files.pythonhosted.org/packages/ef/dd/00e90bd1f6788f06ca5ea83a0ec8dd76350b38303bb8f09d2bf692eb1294/google_cloud_core-0.25.0-py2.py3-none-any.whl\n",
      "Collecting gapic-google-cloud-pubsub-v1<0.16dev,>=0.15.0 (from google-cloud-pubsub==0.26.0; extra == \"gcp\"->apache-beam[gcp])\n",
      "Requirement already satisfied, skipping upgrade: ordereddict in /usr/local/envs/py2env/lib/python2.7/site-packages (from funcsigs>=1->mock<3.0.0,>=1.0.1->apache-beam[gcp]) (1.1)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]) (2.6)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]) (1.22)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]) (2018.10.15)\n",
      "Collecting monotonic>=0.1 (from fasteners>=0.14->google-apitools<=0.5.20,>=0.5.18; extra == \"gcp\"->apache-beam[gcp])\n",
      "  Using cached https://files.pythonhosted.org/packages/ac/aa/063eca6a416f397bd99552c534c6d11d57f58f2e94c14780f3bbf818c4cf/monotonic-1.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: google-auth-httplib2 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-cloud-core<0.26dev,>=0.25.0->google-cloud-bigquery==0.25.0; extra == \"gcp\"->apache-beam[gcp]) (0.0.3)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2.0.0dev,>=0.4.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-cloud-core<0.26dev,>=0.25.0->google-cloud-bigquery==0.25.0; extra == \"gcp\"->apache-beam[gcp]) (1.5.1)\n",
      "Collecting grpc-google-iam-v1<0.12dev,>=0.11.1 (from gapic-google-cloud-pubsub-v1<0.16dev,>=0.15.0->google-cloud-pubsub==0.26.0; extra == \"gcp\"->apache-beam[gcp])\n",
      "Collecting google-gax<0.16dev,>=0.15.7 (from gapic-google-cloud-pubsub-v1<0.16dev,>=0.15.0->google-cloud-pubsub==0.26.0; extra == \"gcp\"->apache-beam[gcp])\n",
      "  Using cached https://files.pythonhosted.org/packages/4f/b4/ff312fa42f91535c67567c1d08e972db0e7c548e9a63c6f3bcc5213b32fc/google_gax-0.15.16-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-auth<2.0.0dev,>=0.4.0->google-cloud-core<0.26dev,>=0.25.0->google-cloud-bigquery==0.25.0; extra == \"gcp\"->apache-beam[gcp]) (2.1.0)\n",
      "Collecting ply==3.8 (from google-gax<0.16dev,>=0.15.7->gapic-google-cloud-pubsub-v1<0.16dev,>=0.15.0->google-cloud-pubsub==0.26.0; extra == \"gcp\"->apache-beam[gcp])\n",
      "Installing collected packages: fastavro, typing, pydot, pyvcf, docopt, hdfs, pytz, monotonic, fasteners, google-apitools, proto-google-cloud-pubsub-v1, google-cloud-core, google-cloud-bigquery, grpc-google-iam-v1, ply, google-gax, gapic-google-cloud-pubsub-v1, google-cloud-pubsub, apache-beam\n",
      "  Found existing installation: pytz 2018.5\n",
      "    Uninstalling pytz-2018.5:\n",
      "      Successfully uninstalled pytz-2018.5\n",
      "  Found existing installation: google-apitools 0.5.10\n",
      "    Uninstalling google-apitools-0.5.10:\n",
      "      Successfully uninstalled google-apitools-0.5.10\n",
      "  Found existing installation: google-cloud-core 0.28.1\n",
      "    Uninstalling google-cloud-core-0.28.1:\n",
      "      Successfully uninstalled google-cloud-core-0.28.1\n",
      "  Found existing installation: google-cloud-bigquery 0.23.0\n",
      "    Uninstalling google-cloud-bigquery-0.23.0:\n",
      "      Successfully uninstalled google-cloud-bigquery-0.23.0\n",
      "Successfully installed apache-beam-2.7.0 docopt-0.6.2 fastavro-0.19.7 fasteners-0.14.1 gapic-google-cloud-pubsub-v1-0.15.4 google-apitools-0.5.20 google-cloud-bigquery-0.25.0 google-cloud-core-0.25.0 google-cloud-pubsub-0.26.0 google-gax-0.15.16 grpc-google-iam-v1-0.11.4 hdfs-2.1.0 monotonic-1.5 ply-3.8 proto-google-cloud-pubsub-v1-0.15.4 pydot-1.2.4 pytz-2018.4 pyvcf-0.6.8 typing-3.6.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.10\n",
      "  latest version: 4.5.11\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\r",
      "certifi-2018.10.15   | 139 KB    |            |   0% \r",
      "certifi-2018.10.15   | 139 KB    | ########## | 100% \n",
      "\r",
      "pytz-2018.5          | 231 KB    |            |   0% \r",
      "pytz-2018.5          | 231 KB    | #########  |  91% \r",
      "pytz-2018.5          | 231 KB    | ########## | 100% \n",
      "pandas-gbq 0.3.0 has requirement google-cloud-bigquery>=0.28.0, but you'll have google-cloud-bigquery 0.25.0 which is incompatible.\n",
      "google-cloud-monitoring 0.28.0 has requirement google-cloud-core<0.29dev,>=0.28.0, but you'll have google-cloud-core 0.25.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source activate py2env\n",
    "conda install -y pytz\n",
    "pip uninstall -y google-cloud-dataflow\n",
    "pip install --upgrade apache-beam[gcp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'rostlab-181304'    # CHANGE THIS\n",
    "BUCKET = 'rostlab-181304-ml' # REPLACE WITH YOUR BUCKET NAME. Use a regional bucket in the region you selected.\n",
    "REGION = 'us-central1' # Choose an available region for Cloud MLE from https://cloud.google.com/ml-engine/docs/regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datalab.bigquery as bq\n",
    "import os\n",
    "import shutil\n",
    "import apache_beam as beam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# for bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "\n",
    "## ensure we're using python2 env\n",
    "os.environ['CLOUDSDK_PYTHON'] = 'python2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Your current Cloud SDK version is: 212.0.0\n",
      "You will be upgraded to version: 221.0.0\n",
      "\n",
      "+----------------------------------------------------------------------------+\n",
      "|                     These components will be updated.                      |\n",
      "+-----------------------------------------------------+------------+---------+\n",
      "|                         Name                        |  Version   |   Size  |\n",
      "+-----------------------------------------------------+------------+---------+\n",
      "| BigQuery Command Line Tool                          |     2.0.35 | < 1 MiB |\n",
      "| BigQuery Command Line Tool (Platform Specific)      |     2.0.34 | < 1 MiB |\n",
      "| Cloud SDK Core Libraries                            | 2018.10.12 | 8.6 MiB |\n",
      "| Cloud SDK Core Libraries (Platform Specific)        | 2018.09.24 | < 1 MiB |\n",
      "| Cloud Storage Command Line Tool                     |       4.34 | 3.5 MiB |\n",
      "| Cloud Storage Command Line Tool (Platform Specific) |       4.34 | < 1 MiB |\n",
      "| gcloud Alpha Commands                               | 2018.09.04 | < 1 MiB |\n",
      "| gcloud cli dependencies                             | 2018.09.28 | 2.4 MiB |\n",
      "+-----------------------------------------------------+------------+---------+\n",
      "\n",
      "The following release notes are new in this upgrade.\n",
      "Please read carefully for information about new features, breaking changes,\n",
      "and bugs fixed.  The latest full release notes can be viewed at:\n",
      "  https://cloud.google.com/sdk/release_notes\n",
      "\n",
      "221.0.0 (2018-10-16)\n",
      "  Breaking Changes\n",
      "      o **(Cloud SQL)** Removed the deprecated host positional argument from\n",
      "        gcloud sql users <create|delete|set-password>.\n",
      "\n",
      "  App Engine\n",
      "      o Fixed a bug where environment variables with values of 'on' or 'off'\n",
      "        were ending up as 'true' or 'false' when deployed.\n",
      "      o gcloud app logs <read|tail> now displays stdout and stderr from the\n",
      "        App Engine standard environment Python 3.7, PHP 7.2, Go 1.11, Java 8,\n",
      "        and Node.js 8 runtimes by default, or explicitly by supplying\n",
      "        --logs=<stdout|stderr>.\n",
      "\n",
      "  BigQuery\n",
      "      o Added flags for setting a default table partition expiration for a\n",
      "        dataset.\n",
      "\n",
      "  Cloud Dataproc\n",
      "      o Promoted the --no-address flag of gcloud dataproc clusters create and\n",
      "    gcloud dataproc workflow-templates set-managed-cluster to GA.\n",
      "\n",
      "  Cloud SQL\n",
      "      o Updated the cloud_sql_proxy component to version 1.13.0. Please visit\n",
      "        the following release notes for details:\n",
      "        https://github.com/GoogleCloudPlatform/cloudsql-proxy/releases\n",
      "\n",
      "  Cloud Spanner\n",
      "      o Added --enable-partitioned-dml flag for gcloud beta spanner\n",
      "        execute-sql\n",
      "\n",
      "  Compute Engine\n",
      "      o Added gcloud beta compute instances get-guest-attributes command for\n",
      "        retrieving guest attributes.\n",
      "      o Promoted --response flag of gcloud compute health-checks for HTTP/S\n",
      "        commands to GA.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "220.0.0 (2018-10-09)\n",
      "  Breaking Changes\n",
      "      o **(Kubernetes Engine)** Modified output of gcloud container clusters\n",
      "        list for DEGRADED clusters to include reason for degradation.\n",
      "      o **(Kubernetes Engine)** Starting in 1.12, new node pools (and default\n",
      "        node pools in new clusters) will be created with their legacy Compute\n",
      "        Engine instance metadata APIs disabled by default.\n",
      "        * To create a new node pool (or default pool in a new cluster) with\n",
      "          legacy metadata APIs disabled, use the flag --metadata\n",
      "          disable-legacy-endpoints=true. See:\n",
      "          <https://cloud.google.com/kubernetes-engine/docs/how-to/protecting-cluster-metadata#disable-legacy-apis>\n",
      "\n",
      "  Cloud SDK\n",
      "      o Updated the storage/chunk_size property. Commands that upload to\n",
      "        Google Cloud Storage can now control the upload/download chunksize\n",
      "        using this property.\n",
      "      o Some commands no longer call gsutil in their implementation in order\n",
      "        to support Python 3. The gsutil implementation is now deprecated. Use\n",
      "        the storage/use_gsutil property to temporarily get this behavior back.\n",
      "        This property and its old implementation will eventually be removed.\n",
      "        The following commands are affected by this change:\n",
      "        * functions deploy\n",
      "        * compute images import\n",
      "        * dataproc jobs submit <pyspark|hadoop|pig|hive|spark|spark-sql>\n",
      "        * composer environments storage <dags|data|plugins>\n",
      "          <delete|export|import>\n",
      "      o Added functionality to gcloud beta help that allows running a search\n",
      "        for terms of interest within the help text of gcloud commands. For more\n",
      "        information, run $ gcloud beta help --help.\n",
      "\n",
      "  App Engine\n",
      "      o Updated the Java SDK to version 1.9.66. Please visit the following\n",
      "        release notes for details:\n",
      "        <https://cloud.google.com/appengine/docs/java/release-notes>\n",
      "      o Updated the Python SDK to version 1.9.77. Please visit the following\n",
      "        release notes for details:\n",
      "        <https://cloud.google.com/appengine/docs/python/release-notes>\n",
      "\n",
      "  Cloud Build\n",
      "      o Added a warning message to gcloud builds submit for builds submitted\n",
      "        with\n",
      "    configs that don't specify a logging option. See\n",
      "    <https://cloud.google.com/cloud-build/docs/api/reference/rest/v1/projects.builds#loggingmode>.\n",
      "\n",
      "  Cloud Composer\n",
      "      o Added the --python-version flag to gcloud beta composer environments\n",
      "        create to specify the Python version used within the created\n",
      "        environment.\n",
      "\n",
      "  Cloud Dataproc\n",
      "      o Added the --gce-pd-kms-key flag to dataproc clusters create to enable\n",
      "        protecting clusters with Google Cloud KMS encryption.\n",
      "\n",
      "  Cloud Key Management Service\n",
      "      o Modified gcloud kms locations list to display information about the\n",
      "        availability of Hardware Security Modules in each location.\n",
      "\n",
      "  Cloud Spanner\n",
      "      o Updated gcloud spanner execute-sql to accept DML statements.\n",
      "\n",
      "  Compute Engine\n",
      "      o Added support for managed ssl certificates to gcloud beta compute\n",
      "        ssl-certificates.\n",
      "\n",
      "  Internet of Things\n",
      "      o Added --log-level flag for gcloud iot <devices|registries>\n",
      "        <create|update> for alpha and beta.\n",
      "      o Added gcloud iot commands to alpha and beta.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "219.0.1 (2018-10-02)\n",
      "  Cloud SDK\n",
      "      o Updated to a new version of ruamel that fixes Unicode issues\n",
      "        (https://issuetracker.google.com/issues/113348923) on OS X and Windows.\n",
      "\n",
      "  App Engine\n",
      "      o Updated the Python SDK to version 1.9.76. Please visit the following\n",
      "        release notes for details:\n",
      "        <https://cloud.google.com/appengine/docs/python/release-notes>\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted compute routers nats to beta.\n",
      "      o Promoted 'gcloud compute interconnects get-diagnostics' to beta.\n",
      "      o Promoted the following flags to GA to support using KMS keys to\n",
      "        protect disks and images:\n",
      "        * --kms-key- flags of gcloud compute <disks|images>\n",
      "        * --boot-disk-kms- flags of gcloud compute\n",
      "          <instances|instance-templates>.\n",
      "      o Promoted gcloud compute instance-templates create-with-container from\n",
      "        beta to GA.\n",
      "\n",
      "  Interactive\n",
      "      o Promoted gcloud interactive to beta.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Added --enable-private-nodes, --enable-private-endpoint, and\n",
      "        --master-ipv4-cidr flags to gcloud container clusters create.\n",
      "      o Added --internal-ip flag to gcloud container clusters\n",
      "        get-credentials.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "218.0.0 (2018-09-25)\n",
      "  Breaking Changes\n",
      "      o **(Cloud SQL)** Added the PRIVATE_NETWORK column to the gcloud sql\n",
      "        instances list default format and renamed ADDRESS to PRIMARY_ADDRESS.\n",
      "\n",
      "  Cloud Dataproc\n",
      "      o Added gcloud beta dataproc clusters export to enable exporting a\n",
      "        cluster's configuration to a YAML file.\n",
      "      o Added gcloud beta dataproc clusters import to enable creating a\n",
      "        cluster from configuration in a YAML file.\n",
      "      o Added --optional-components flag to gcloud beta dataproc clusters\n",
      "        create command.\n",
      "      o Promoted gcloud dataproc\n",
      "        <clusters|jobs|operations|workflow-templates> <get|set>-iam-policy to\n",
      "        GA.\n",
      "      o Promoted gcloud dataproc workflow-templates to GA.\n",
      "\n",
      "  Cloud Scheduler\n",
      "      o Promoted gcloud scheduler to beta.\n",
      "\n",
      "  Cloud Tools For PowerShell\n",
      "      o Updated Cloud Tools for PowerShell to version 1.0.1.10.\n",
      "        * Fixed a bug where fixed key metadata did not work with Google Cloud\n",
      "          Storage cmdlets.\n",
      "\n",
      "  Compute Engine\n",
      "      o Added the --storage-location flag to gcloud beta compute disks\n",
      "        snapshot\n",
      "    to specify location region of a snapshot.\n",
      "\n",
      "  Container Analysis\n",
      "      o Promoted the following alpha flags in gcloud container images\n",
      "        describe to beta.\n",
      "        * --metadata-filter\n",
      "        * --show-build-details\n",
      "        * --show-package-vulnerability\n",
      "        * --show-image-basis\n",
      "        * --show-deployment\n",
      "        * --show-all-metadata\n",
      "      o Promoted the following alpha flags in gcloud container images\n",
      "        list-tags to beta.\n",
      "        * --occurrence-filter\n",
      "        * --show-occurrences\n",
      "        * --show-occurrences-from For more information about occurrences, see\n",
      "          <https://cloud.google.com/container-registry/docs/container-analysis>.\n",
      "\n",
      "  Firebase Test Lab\n",
      "      o Fixed bug where --environment-variables did not work in the 217.0.0\n",
      "        release.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Added --metadata and --metadata-from-file flags to gcloud\n",
      "        <node-pools|clusters> create.\n",
      "      o Added --internal-ip flag to gcloud beta container clusters\n",
      "        get-credentials.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "217.0.0 (2018-09-18)\n",
      "  Breaking Changes\n",
      "      o **(Cloud Services)** Renamed --reserved-ranges to ranges in gcloud\n",
      "        beta services vpc-peerings.\n",
      "\n",
      "  App Engine\n",
      "      o Updated the Java SDK to version 1.9.65. Please visit the following\n",
      "        release notes for details:\n",
      "        https://cloud.google.com/appengine/docs/java/release-notes\n",
      "\n",
      "  Cloud SQL\n",
      "      o Promoted the --network flag of gcloud sql instances <create|patch> to\n",
      "        beta.\n",
      "\n",
      "  Cloud Storage\n",
      "      o Updated gsutil component to 4.34\n",
      "\n",
      "  Compute Engine\n",
      "      o Added --prefix-length, --purpose and --network flags to BETA for\n",
      "        gcloud compute addresses create to support reserving IP range from\n",
      "        virtual network for peering.\n",
      "      o Added the description property to the --create-disk flag of gcloud\n",
      "        compute <instances|instance-templates> create.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Added --metadata and --metadata-from-file flags to gcloud beta\n",
      "        <node-pools|clusters> create.\n",
      "      o Updated Google Kubernetes Engine's kubectl from version 1.9.7 to\n",
      "        1.10.7.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "216.0.0 (2018-09-11)\n",
      "  App Engine\n",
      "      o Updated the Go SDK to version 1.9.68. Please visit the following\n",
      "        release notes for details:\n",
      "        https://cloud.google.com/appengine/docs/go/release-notes\n",
      "\n",
      "  Cloud Bigtable\n",
      "      o Promoted the following commands to GA:\n",
      "        * gcloud bigtable instances command group\n",
      "        * gcloud bigtable clusters list\n",
      "        * gcloud bigtable clusters describe\n",
      "\n",
      "  Cloud Memorystore\n",
      "      o Promoted gcloud redis to GA.\n",
      "\n",
      "  Cloud Services\n",
      "      o Deprecated gcloud services operations list in beta and GA.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted --internal-ip flag of gcloud compute scp to beta.\n",
      "      o Promoted --disabled flag of gcloud compute firewall-rules to GA.\n",
      "      o Fixed bug preventing gcloud compute <ssh|scp> from finding an\n",
      "        instance's external IP address when configured with multiple network\n",
      "        interfaces.\n",
      "      o Promoted gcloud compute instances\n",
      "        <create-with-container|update-container> to GA.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "215.0.0 (2018-09-05)\n",
      "  Breaking Changes\n",
      "      o **(Cloud SQL)** Removed the default value of the --database-version\n",
      "        flag of gcloud sql instances create, allowing the API to select the\n",
      "        value.\n",
      "\n",
      "  App Engine\n",
      "      o Added python 3 support for gcloud app and gcloud domains.\n",
      "      o Added the --no-cache flag to gcloud beta app deploy for Second\n",
      "        Generation runtimes, to disable the build cache during deployment.\n",
      "      o Updated the Python SDK to version 1.9.75. Please visit the following\n",
      "        release notes for details:\n",
      "        https://cloud.google.com/appengine/docs/python/release-notes\n",
      "\n",
      "  Cloud Datalab\n",
      "      o Updated the datalab component to the 20180823 release. Released\n",
      "        changes are documented in its tracking issue at\n",
      "        https://github.com/googledatalab/datalab/issues/2068\n",
      "        (https://github.com/googledatalab/datalab/issues/2068).\n",
      "\n",
      "  Cloud Dataproc\n",
      "      o Fixed an issue preventing the creation of clusters with SSD in GA.\n",
      "\n",
      "  Cloud Firestore\n",
      "      o Added gcloud beta firestore and gcloud beta firestore operations for\n",
      "        managing cloud firestore imports and exports.\n",
      "\n",
      "  Cloud Functions\n",
      "      o Added --service-account flag to gcloud alpha functions deploy.\n",
      "\n",
      "  Cloud Pub/Sub\n",
      "      o Promoted Snapshot & Seek features to beta. These features allow users\n",
      "        to create snapshots of subscription backlog state, and later restore\n",
      "        that state.\n",
      "\n",
      "  Cloud SQL\n",
      "      o Fixed a bug preventing --backup and --enable-bin-log flags from being\n",
      "        sent together when using the gcloud sql instances create command.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted --internal-ip flag of gcloud compute scp to beta.\n",
      "      o Promoted --disabled flag of gcloud compute firewall-rules to GA.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Promote gcloud alpha container subnets list-usable to Beta.\n",
      "      o Add secondaryIpRanges to the output of gcloud beta container subnets\n",
      "        list-usable.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "214.0.0 (2018-08-28)\n",
      "  Breaking Changes\n",
      "      o **(Cloud Bigtable)** Modified the arguments accepted by cbt\n",
      "        createappprofile and cbt updateappprofile in the following ways:\n",
      "        * Removed etag argument from createappprofile.\n",
      "        * Renamed allow-transactional-writes option as transactional-writes.\n",
      "        * Added a force option to ignore warnings.\n",
      "      o **(Cloud Bigtable)** Modified the specification for routing policies.\n",
      "        A routing policy can be either \"route-any\" (previously of\n",
      "        \"multi_cluster_routing_use_any\") or \"route-to=<cluster-id>\".\n",
      "      o **(Compute Engine)** Deprecated gcloud compute interconnects\n",
      "        attachments create. Please use gcloud compute interconnects attachments\n",
      "        dedicated create instead.\n",
      "      o **(Compute Engine)** Removed deprecated --mode flag from gcloud\n",
      "        compute networks create. Use --subnet-mode instead.\n",
      "      o **(Compute Engine)** Removed deprecated gcloud compute networks\n",
      "        switch-mode command. Use gcloud compute networks update\n",
      "        --switch-to-custom-mode instead.\n",
      "      o **(Compute Engine)** Removed deprecated gcloud compute xpn command\n",
      "        group. Use gcloud compute shared-vpc instead.\n",
      "\n",
      "  Cloud Bigtable\n",
      "      o Restored the output of the cbt count command that was inadvertently\n",
      "        removed in the previous release.\n",
      "\n",
      "  Cloud Datalab\n",
      "      o Updated the datalab component to the 20180820 release. Released\n",
      "        changes are documented in its tracking issue at\n",
      "        https://github.com/googledatalab/datalab/issues/2064\n",
      "        (https://github.com/googledatalab/datalab/issues/2064).\n",
      "\n",
      "  Cloud Dataproc\n",
      "      o Added SCHEDULED_DELETE column to gcloud beta dataproc clusters list\n",
      "        command output.\n",
      "\n",
      "  Cloud Datastore Emulator\n",
      "      o Released Cloud Datastore Emulator version 2.0.2.\n",
      "        * Improved backward compatibility with App Engine local development\n",
      "          by keeping auto generated indexes in index file generated from\n",
      "          previous runs.\n",
      "\n",
      "  Cloud Functions\n",
      "      o Promoted --runtime flag of gcloud functions deploy to GA.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted the following flags to GA:\n",
      "        * --network-tier of gcloud compute <addresses|forwarding-rules>\n",
      "          create\n",
      "        * --default-network-tier of gcloud compute project-info update\n",
      "        * --network-tier of gcloud compute instances\n",
      "          <add-access-config|create>\n",
      "        * --network-tier of gcloud compute instance-templates create\n",
      "      o Promoted gcloud compute instances simulate-maintenance-event to GA.\n",
      "      o Promoted <get|set\\>-iam-policy and <add|remove\\>-iam-policy-bindings\n",
      "        to beta in the following commands groups:\n",
      "        * gcloud compute sole-tenancy node-groups\n",
      "        * gcloud compute sole-tenancy node-templates\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Promoted --disk-type flag of gcloud container <clusters|node-pools>\n",
      "        create to GA.\n",
      "      o Promoted --default-max-pods-per-node flag of gcloud container\n",
      "        clusters create to beta.\n",
      "      o Promoted --max-pods-per-node flag of gcloud container node-pools\n",
      "        create to beta.\n",
      "      o Modified --monitoring-service flag of gcloud containers clusters\n",
      "        update to enable Google Cloud Monitoring service with Kubernetes-native\n",
      "        resource model.\n",
      "      o Modified --logging-service flag of gcloud containers clusters update\n",
      "        to enable Google Cloud Logging service with Kubernetes-native resource\n",
      "        model.\n",
      "      o Modified output of gcloud beta container clusters list for DEGRADED\n",
      "        clusters to include reason for degradation.\n",
      "      o Added --enable-private-nodes and --enable-private-endpoint to gcloud\n",
      "        beta container clusters create.\n",
      "      o Deprecated --private-cluster flag of gcloud beta container clusters\n",
      "        create; use --enable-private-nodes instead.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "213.0.0 (2018-08-21)\n",
      "  Breaking Changes\n",
      "      o **(Cloud Datastore)** Deprecated gcloud datastore\n",
      "        <create|cleanup>-indexes. Use gcloud datastore indexes <create|cleanup>\n",
      "        instead.\n",
      "\n",
      "  App Engine\n",
      "      o Updated the Python SDK to version 1.9.74. Please visit the following\n",
      "        release notes for details:\n",
      "        <https://cloud.google.com/appengine/docs/python/release-notes>\n",
      "\n",
      "  Cloud Datastore\n",
      "      o Promoted gcloud datastore indexes command group to GA.\n",
      "\n",
      "  Cloud SQL\n",
      "      o Added the --timeout flag to gcloud sql operations wait, to support\n",
      "        specifying a custom timeout or allow the command to wait indefinitely.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Promoted --disk-type flag of gcloud container <clusters|node-pools>\n",
      "        create to GA.\n",
      "      o Modified --monitoring-service flag of gcloud containers clusters\n",
      "        update to enable Google Cloud Monitoring service with Kubernetes-native\n",
      "        resource model.\n",
      "      o Modified --logging-service flag of gcloud containers clusters update\n",
      "        to enable Google Cloud Logging service with Kubernetes-native resource\n",
      "        model.\n",
      "      o Modified output of gcloud beta container clusters list for DEGRADED\n",
      "        clusters to include reason for degradation.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "Do you want to continue (Y/n)?  \n",
      "#============================================================#\n",
      "#= Creating update staging area                             =#\n",
      "#============================================================#\n",
      "#= Uninstalling: BigQuery Command Line Tool                 =#\n",
      "#============================================================#\n",
      "#= Uninstalling: BigQuery Command Line Tool (Platform Sp... =#\n",
      "#============================================================#\n",
      "#= Uninstalling: Cloud SDK Core Libraries                   =#\n",
      "#============================================================#\n",
      "#= Uninstalling: Cloud SDK Core Libraries (Platform Spec... =#\n",
      "#============================================================#\n",
      "#= Uninstalling: Cloud Storage Command Line Tool            =#\n",
      "#============================================================#\n",
      "#= Uninstalling: Cloud Storage Command Line Tool (Platfo... =#\n",
      "#============================================================#\n",
      "#= Uninstalling: gcloud Alpha Commands                      =#\n",
      "#============================================================#\n",
      "#= Uninstalling: gcloud cli dependencies                    =#\n",
      "#============================================================#\n",
      "#= Installing: BigQuery Command Line Tool                   =#\n",
      "#============================================================#\n",
      "#= Installing: BigQuery Command Line Tool (Platform Spec... =#\n",
      "#============================================================#\n",
      "#= Installing: Cloud SDK Core Libraries                     =#\n",
      "#============================================================#\n",
      "#= Installing: Cloud SDK Core Libraries (Platform Specific) =#\n",
      "#============================================================#\n",
      "#= Installing: Cloud Storage Command Line Tool              =#\n",
      "#============================================================#\n",
      "#= Installing: Cloud Storage Command Line Tool (Platform... =#\n",
      "#============================================================#\n",
      "#= Installing: gcloud Alpha Commands                        =#\n",
      "#============================================================#\n",
      "#= Installing: gcloud cli dependencies                      =#\n",
      "#============================================================#\n",
      "#= Creating backup and activating new installation          =#\n",
      "#============================================================#\n",
      "\n",
      "Performing post processing steps...\n",
      "....................................done.\n",
      "\n",
      "==> Start a new shell for the changes to take effect.\n",
      "\n",
      "\n",
      "Update done!\n",
      "\n",
      "To revert your SDK to the previously installed version, you may run:\n",
      "  $ gcloud components update --version 212.0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## ensure gcloud is up to date\n",
    "gcloud components update\n",
    "\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION\n",
    "\n",
    "## ensure we predict locally with our current Python environment\n",
    "gcloud config set ml_engine/local_python `which python`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying query to pull the data\n",
    "\n",
    "Let's pull out a few extra columns from the timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT\n",
      "  (tolls_amount + fare_amount) AS fare_amount,\n",
      "  DAYOFWEEK(pickup_datetime) AS dayofweek,\n",
      "  HOUR(pickup_datetime) AS hourofday,\n",
      "  pickup_longitude AS pickuplon,\n",
      "  pickup_latitude AS pickuplat,\n",
      "  dropoff_longitude AS dropofflon,\n",
      "  dropoff_latitude AS dropofflat,\n",
      "  passenger_count*1.0 AS passengers,\n",
      "  CONCAT(STRING(pickup_datetime), STRING(pickup_longitude), STRING(pickup_latitude), STRING(dropoff_latitude), STRING(dropoff_longitude)) AS key\n",
      "FROM\n",
      "  [nyc-tlc:yellow.trips]\n",
      "WHERE\n",
      "  trip_distance > 0\n",
      "  AND fare_amount >= 2.5\n",
      "  AND pickup_longitude > -78\n",
      "  AND pickup_longitude < -70\n",
      "  AND dropoff_longitude > -78\n",
      "  AND dropoff_longitude < -70\n",
      "  AND pickup_latitude > 37\n",
      "  AND pickup_latitude < 45\n",
      "  AND dropoff_latitude > 37\n",
      "  AND dropoff_latitude < 45\n",
      "  AND passenger_count > 0\n",
      "   AND ABS(HASH(pickup_datetime)) % 100 == 2\n"
     ]
    }
   ],
   "source": [
    "def create_query(phase, EVERY_N):\n",
    "  if EVERY_N == None:\n",
    "    EVERY_N = 4 #use full dataset\n",
    "    \n",
    "  #select and pre-process fields\n",
    "  base_query = \"\"\"\n",
    "SELECT\n",
    "  (tolls_amount + fare_amount) AS fare_amount,\n",
    "  DAYOFWEEK(pickup_datetime) AS dayofweek,\n",
    "  HOUR(pickup_datetime) AS hourofday,\n",
    "  pickup_longitude AS pickuplon,\n",
    "  pickup_latitude AS pickuplat,\n",
    "  dropoff_longitude AS dropofflon,\n",
    "  dropoff_latitude AS dropofflat,\n",
    "  passenger_count*1.0 AS passengers,\n",
    "  CONCAT(STRING(pickup_datetime), STRING(pickup_longitude), STRING(pickup_latitude), STRING(dropoff_latitude), STRING(dropoff_longitude)) AS key\n",
    "FROM\n",
    "  [nyc-tlc:yellow.trips]\n",
    "WHERE\n",
    "  trip_distance > 0\n",
    "  AND fare_amount >= 2.5\n",
    "  AND pickup_longitude > -78\n",
    "  AND pickup_longitude < -70\n",
    "  AND dropoff_longitude > -78\n",
    "  AND dropoff_longitude < -70\n",
    "  AND pickup_latitude > 37\n",
    "  AND pickup_latitude < 45\n",
    "  AND dropoff_latitude > 37\n",
    "  AND dropoff_latitude < 45\n",
    "  AND passenger_count > 0\n",
    "  \"\"\"\n",
    "  \n",
    "  #add subsampling criteria by modding with hashkey\n",
    "  if phase == 'train': \n",
    "    query = \"{} AND ABS(HASH(pickup_datetime)) % {} < 2\".format(base_query,EVERY_N)\n",
    "  elif phase == 'valid': \n",
    "    query = \"{} AND ABS(HASH(pickup_datetime)) % {} == 2\".format(base_query,EVERY_N)\n",
    "  elif phase == 'test':\n",
    "    query = \"{} AND ABS(HASH(pickup_datetime)) % {} == 3\".format(base_query,EVERY_N)\n",
    "  return query\n",
    "    \n",
    "print(create_query('valid', 100)) #example query using 1% of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Dataflow job from BigQuery This code reads from BigQuery and saves the data as-is on Google Cloud Storage. We can do additional preprocessing and cleanup inside Dataflow, but then we'll have to remember to repeat that prepreprocessing during inference. It is better to use tf.transform which will do this book-keeping for you, or to do preprocessing within your TensorFlow model. We will look at this in future notebooks. For now, we are simply moving data from BigQuery to CSV using Dataflow.\n",
    "While we could read from BQ directly from TensorFlow (See: https://www.tensorflow.org/api_docs/python/tf/contrib/cloud/BigQueryReader), it is quite convenient to export to CSV and do the training off CSV. Let's use Dataflow to do this at scale.\n",
    "\n",
    "Because we are running this on the Cloud, you should go to the GCP Console (https://console.cloud.google.com/dataflow) to look at the status of the job. It will take several minutes for the preprocessing job to launch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if gsutil ls | grep -q gs://${BUCKET}/taxifare/taxi_preproc/; then\n",
    "  gsutil -m rm -rf gs://$BUCKET/taxifare/taxi_preproc/\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "####\n",
    "# Arguments:\n",
    "#   -rowdict: Dictionary. The beam bigquery reader returns a PCollection in\n",
    "#     which each row is represented as a python dictionary\n",
    "# Returns:\n",
    "#   -rowstring: a comma separated string representation of the record with dayofweek\n",
    "#     converted from int to string (e.g. 3 --> Tue)\n",
    "####\n",
    "def to_csv(rowdict):\n",
    "  days = ['null', 'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n",
    "  CSV_COLUMNS = 'fare_amount,dayofweek,hourofday,pickuplon,pickuplat,dropofflon,dropofflat,passengers,key'.split(',')\n",
    "  rowdict['dayofweek'] = days[rowdict['dayofweek']]\n",
    "  rowstring = ','.join([str(rowdict[k]) for k in CSV_COLUMNS])\n",
    "  return rowstring\n",
    "\n",
    "\n",
    "####\n",
    "# Arguments:\n",
    "#   -EVERY_N: Integer. Sample one out of every N rows from the full dataset.\n",
    "#     Larger values will yield smaller sample\n",
    "#   -RUNNER: 'DirectRunner' or 'DataflowRunner'. Specfy to run the pipeline\n",
    "#     locally or on Google Cloud respectively. \n",
    "# Side-effects:\n",
    "#   -Creates and executes dataflow pipeline. \n",
    "#     See https://beam.apache.org/documentation/programming-guide/#creating-a-pipeline\n",
    "####\n",
    "def preprocess(EVERY_N, RUNNER):\n",
    "  job_name = 'preprocess-taxifeatures' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "  print('Launching Dataflow job {} ... hang on'.format(job_name))\n",
    "  OUTPUT_DIR = 'gs://{0}/taxifare/taxi_preproc/'.format(BUCKET)\n",
    "\n",
    "  #dictionary of pipeline options\n",
    "  options = {\n",
    "    'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "    'job_name': 'preprocess-taxifeatures' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S'),\n",
    "    'project': PROJECT,\n",
    "    'runner': RUNNER\n",
    "  }\n",
    "  #instantiate PipelineOptions object using options dictionary\n",
    "  opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "  #instantantiate Pipeline object using PipelineOptions\n",
    "  with beam.Pipeline(options=opts) as p:\n",
    "      for phase in ['train', 'valid']:\n",
    "        query = create_query(phase, EVERY_N) \n",
    "        outfile = os.path.join(OUTPUT_DIR, '{}.csv'.format(phase))\n",
    "        (\n",
    "          p | 'read_{}'.format(phase) >> beam.io.Read(beam.io.BigQuerySource(query=query))\n",
    "            | 'tocsv_{}'.format(phase) >> beam.Map(to_csv)\n",
    "            | 'write_{}'.format(phase) >> beam.io.Write(beam.io.WriteToText(outfile))\n",
    "        )\n",
    "  print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run pipeline locally. This takes up to <b>5 minutes</b>.  You will see a message \"Done\" when it is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Dataflow job preprocess-taxifeatures-181021-041751 ... hang on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Dataset rostlab-181304:temp_dataset_07b746fb139e4b97b88c62cefaae570d does not exist so we will create it as temporary with location=None\n",
      "WARNING:root:Dataset rostlab-181304:temp_dataset_f98d0f1de7be459ea867d010c5452eae does not exist so we will create it as temporary with location=None\n",
      "WARNING:root:Deleting 2 existing files in target path matching: -*-of-%(num_shards)05d\n",
      "WARNING:root:Deleting 5 existing files in target path matching: -*-of-%(num_shards)05d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "preprocess(50*10000, 'DirectRunner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  48668142  2018-10-21T04:46:10Z  gs://rostlab-181304-ml/taxifare/taxi_preproc/train.csv-00000-of-00001\n",
      "  38751674  2018-10-21T05:20:48Z  gs://rostlab-181304-ml/taxifare/taxi_preproc/train.csv-00000-of-00002\n",
      "  83124859  2018-10-21T05:20:48Z  gs://rostlab-181304-ml/taxifare/taxi_preproc/train.csv-00001-of-00002\n",
      "  61397578  2018-10-21T05:19:49Z  gs://rostlab-181304-ml/taxifare/taxi_preproc/valid.csv-00000-of-00001\n",
      "                                 gs://rostlab-181304-ml/taxifare/taxi_preproc/tmp/\n",
      "TOTAL: 4 objects, 231942253 bytes (221.2 MiB)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls -l gs://$BUCKET/taxifare/taxi_preproc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Beam pipeline on Cloud Dataflow\n",
    "Run pipleline on cloud on a larger sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if gsutil ls | grep -q gs://${BUCKET}/taxifare/taxi_preproc/; then\n",
    "  gsutil -m rm -rf gs://$BUCKET/taxifare/taxi_preproc/\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Dataflow job preprocess-taxifeatures-181021-051247 ... hang on\n"
     ]
    }
   ],
   "source": [
    "preprocess(20*100, 'DataflowRunner') \n",
    "\n",
    "#change first arg to None to preprocess full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job completes, observe the files created in Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  48668142  2018-10-21T04:46:10Z  gs://rostlab-181304-ml/taxifare/taxi_preproc/train.csv-00000-of-00001\n",
      "  38751674  2018-10-21T05:20:48Z  gs://rostlab-181304-ml/taxifare/taxi_preproc/train.csv-00000-of-00002\n",
      "  83124859  2018-10-21T05:20:48Z  gs://rostlab-181304-ml/taxifare/taxi_preproc/train.csv-00001-of-00002\n",
      "  61397578  2018-10-21T05:19:49Z  gs://rostlab-181304-ml/taxifare/taxi_preproc/valid.csv-00000-of-00001\n",
      "                                 gs://rostlab-181304-ml/taxifare/taxi_preproc/tmp/\n",
      "TOTAL: 4 objects, 231942253 bytes (221.2 MiB)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls -l gs://$BUCKET/taxifare/taxi_preproc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.5,Thu,22,-73.98067,40.759296,-73.95608,40.773807,1.0,2013-04-04 22:58:55.000000-73.980740.759340.7738-73.9561\n",
      "6.5,Thu,22,-73.980478,40.754022,-73.961178,40.768916,2.0,2012-04-26 22:46:41.000000-73.980540.75440.7689-73.9612\n",
      "16.5,Thu,22,-73.9824447632,40.7675895691,-73.9795074463,40.7259635925,3.0,2015-04-16 22:56:18.000000-73.982440.767640.726-73.9795\n",
      "7.3,Thu,22,-73.954668,40.780712,-73.975593,40.778688,1.0,2010-04-15 22:11:00.000000-73.954740.780740.7787-73.9756\n",
      "6.1,Thu,22,-73.991701,40.7512,-73.975416,40.746262,1.0,2010-11-25 22:05:46.000000-73.991740.751240.7463-73.9754\n",
      "17.0,Thu,22,-73.991871,40.754238,-73.991871,40.754238,1.0,2013-05-30 22:58:09.000000-73.991940.754240.7542-73.9919\n",
      "11.3,Thu,22,-74.002235,40.740053,-73.973771,40.789713,1.0,2011-03-24 22:22:09.000000-74.002240.740140.7897-73.9738\n",
      "9.0,Thu,22,-73.99683,40.725467,-73.981288,40.74436,5.0,2013-06-06 22:19:00.000000-73.996840.725540.7444-73.9813\n",
      "5.7,Thu,22,-73.954177,40.787283,-73.969479,40.785209,2.0,2010-06-17 22:06:16.000000-73.954240.787340.7852-73.9695\n",
      "11.5,Thu,22,-73.98922,40.747687,-73.95206,40.773005,2.0,2013-02-14 22:11:00.000000-73.989240.747740.773-73.9521\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#print first 10 lines of first shard of train.csv\n",
    "gsutil cat \"gs://$BUCKET/taxifare/taxi_preproc/train.csv-00000-of-*\" | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
